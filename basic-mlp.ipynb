{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "57832092-a908-442c-8f6d-0eae401d69ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5a44c321-2136-4dcb-af93-9d7813e05be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the very basic value class upon which activations are performed, and can backpropagate through to their parents\n",
    "# we store each value as a member of a DAG with the gradient propagating through its parents\n",
    "class Value():\n",
    "    def __init__(self, data, _op='', _parents=(), ):\n",
    "        self._op = _op\n",
    "        self._backprop = lambda: None\n",
    "        self.grad = 0.0\n",
    "        self.data = data\n",
    "        self._parents = _parents       \n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data})\"        \n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data + other.data, _op='+', _parents=(self, other))\n",
    "        def _backprop():\n",
    "            delta = out.grad\n",
    "            self.grad += 1.0*delta\n",
    "            other.grad += 1.0*delta\n",
    "        out._backprop = _backprop\n",
    "        return out\n",
    "    def __radd__(self, other):\n",
    "        return self + other\n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data * other.data, _op='*', _parents=(self, other))\n",
    "        def _backprop():\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad\n",
    "        out._backprop = _backprop\n",
    "        return out\n",
    "    def __rmul__(self, other):\n",
    "        return self * other\n",
    "    def __neg__(self):\n",
    "        return (-1)*self;      \n",
    "    def __sub__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        return self + (-other)\n",
    "    def __rsub__(self, other):\n",
    "        return (-self) + (other)\n",
    "    def __pow__(self, other):\n",
    "        assert isinstance(other, (int, float))\n",
    "        out = Value(self.data**other, _parents=(self, ), _op=f\"^{other}\")\n",
    "        def _backprop():\n",
    "            self.grad += other*(self.data**(other - 1))*out.grad\n",
    "        out._backprop = _backprop\n",
    "        return out\n",
    "    def __truediv__(self, other):\n",
    "        return self*(other**-1)\n",
    "    #unipolar\n",
    "    def sigmoid(self):\n",
    "        x = self.data\n",
    "        sig = 1 / (1 + math.exp(-x))\n",
    "        out = Value(sig, _op='sig', _parents=(self, ))\n",
    "        # the derivative of o = sigmoid is o*(1 - o)\n",
    "        def _backprop():\n",
    "            delta = out.grad * (1 - sig) * sig\n",
    "            self.grad += delta\n",
    "        out._backprop = _backprop        \n",
    "        return out  \n",
    "    def tanh(self):\n",
    "        x = self.data\n",
    "        t = (math.exp(2*x) - 1) / (math.exp(2*x) + 1)\n",
    "        out = Value(t, _op='tanh', _parents=(self, ))\n",
    "        def _backprop():\n",
    "            delta = out.grad * (1 - t**2)\n",
    "            self.grad += delta\n",
    "        out._backprop = _backprop\n",
    "        return out\n",
    "    def backprop(self):\n",
    "\n",
    "        # toposort\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def topo_sort(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for parent in v._parents:\n",
    "                    topo_sort(parent)\n",
    "                topo.append(v)\n",
    "        topo_sort(self)\n",
    "        self.grad = 1\n",
    "        for node in reversed(topo):\n",
    "            node._backprop()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9df0af19-69f4-482d-91f2-68fb908cb7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "def trace(root):\n",
    "  # builds a set of all nodes and edges in a graph\n",
    "  nodes, edges = set(), set()\n",
    "  def build(v):\n",
    "    if v not in nodes:\n",
    "      nodes.add(v)\n",
    "      for child in v._parents:\n",
    "        edges.add((child, v))\n",
    "        build(child)\n",
    "  build(root)\n",
    "  return nodes, edges\n",
    "\n",
    "def draw_dot(root):\n",
    "  dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) # LR = left to right\n",
    "\n",
    "  nodes, edges = trace(root)\n",
    "  for n in nodes:\n",
    "    uid = str(id(n))\n",
    "    # for any value in the graph, create a rectangular ('record') node for it\n",
    "    dot.node(name = uid, label = \"{ data %.4f | grad %.4f }\" % ( n.data, n.grad), shape='record')\n",
    "    if n._op:\n",
    "      # if this value is a result of some operation, create an op node for it\n",
    "      dot.node(name = uid + n._op, label = n._op)\n",
    "      # and connect this node to it\n",
    "      dot.edge(uid + n._op, uid)\n",
    "\n",
    "  for n1, n2 in edges:\n",
    "    # connect n1 to the op node of n2\n",
    "    dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n",
    "\n",
    "  return dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "11436847-6389-402d-82c7-2a16004757f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass in the dimension of input and weights initialized accordingly, single bias\n",
    "class Neuron():\n",
    "    def __init__(self, idim, activation='tanh'):\n",
    "        self.weights = [Value(random.uniform(-1, 1)) for _ in range(idim)]\n",
    "        self.bias = Value(random.uniform(-1, 1))\n",
    "        self.activation = activation\n",
    "    def __repr__(self):\n",
    "        return f\"weights: {self.weights} \\n bias: {self.bias} \\n activation: {self.activation}\"\n",
    "    def parameters(self):\n",
    "        return [self.bias] + self.weights\n",
    "    def forward(self, x):\n",
    "        net = sum((wi*xi for wi, xi in zip(self.weights, x)), self.bias)\n",
    "        activation_func = getattr(net, self.activation, None)\n",
    "        if activation_func is None:\n",
    "            raise ValueError(\"Invalid activation\")\n",
    "        out = activation_func()\n",
    "        return out\n",
    "\n",
    "class Layer():\n",
    "    def __init__(self, idim, odim, activation):\n",
    "        self.neurons = [Neuron(idim, activation) for _ in range(odim)]\n",
    "    def __repr__(self):\n",
    "        final = f\"\"\n",
    "        for n in self.neurons:\n",
    "            final += f\"neuron: {repr(n)}\\n\" \n",
    "        return final\n",
    "    def parameters(self):\n",
    "        return [p for neuron in self.neurons for p in neuron.parameters()]\n",
    "    def forward(self, x):\n",
    "        outvec = [n.forward(x) for n in self.neurons]\n",
    "        return outvec[0] if len(outvec) == 1 else outvec\n",
    "\n",
    "#expects an 2D array representing data points, this is a scalar valued library after all\n",
    "class MLP():\n",
    "    def __init__(self, idim, odims, learning_rate, x_in, y_target, activation='tanh', epochs = 20):\n",
    "        dims = [idim] + odims\n",
    "        self.layers = [Layer(dims[i], dims[i+1], activation) for i in range(len(odims))]\n",
    "        self.x_in = x_in\n",
    "        self.y_target = y_target\n",
    "        self.eta = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.preds = None\n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters()]\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    def train(self):\n",
    "        for i in range(self.epochs):\n",
    "            # make prediction\n",
    "            ypred = [self.forward(x) for x in self.x_in]\n",
    "            \n",
    "            #calculate loss - using a simple squared loss here\n",
    "            loss = sum((yp - yt)**2 for yp, yt in zip(ypred, self.y_target))\n",
    "            # minimize the loss using backprop and bump parameters\n",
    "            loss.backprop()\n",
    "            #update\n",
    "            for p in self.parameters():\n",
    "                p.data -= self.eta*p.grad\n",
    "                p.grad = 0.0\n",
    "            print(f\"Epoch {i}: Loss : {loss.data}\")\n",
    "            self.preds = ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "6ff88d95-6e34-4674-99ba-84d473d80d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "xt = [[2,0,3,0,-1,0],\n",
    "      [3,0,-1.0,0,5],\n",
    "      [0.5,1.0,1.0],\n",
    "      [1.0,1.0,-1.0]]\n",
    "yt = [1.0, -1.0, -1.0, 1.0]\n",
    "mlp = MLP(3, [4, 4, 1], 0.1, xt, yt,'tanh', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d7554061-8732-46cd-ab7c-0d637851b2c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss : 6.383880094225191\n",
      "Epoch 1: Loss : 3.3364119149396636\n",
      "Epoch 2: Loss : 2.572504773854254\n",
      "Epoch 3: Loss : 1.739968060536253\n",
      "Epoch 4: Loss : 1.363422763422875\n",
      "Epoch 5: Loss : 3.9031432807383526\n",
      "Epoch 6: Loss : 6.92839341760446\n",
      "Epoch 7: Loss : 0.9073128227891409\n",
      "Epoch 8: Loss : 0.50851031030276\n",
      "Epoch 9: Loss : 0.20708899328226568\n",
      "Epoch 10: Loss : 0.1618752058337197\n",
      "Epoch 11: Loss : 0.14208324622561164\n",
      "Epoch 12: Loss : 0.12621265974360568\n",
      "Epoch 13: Loss : 0.11317451134998403\n",
      "Epoch 14: Loss : 0.10229027805268434\n",
      "Epoch 15: Loss : 0.09308568453496911\n",
      "Epoch 16: Loss : 0.08521558732236303\n",
      "Epoch 17: Loss : 0.0784220888970609\n",
      "Epoch 18: Loss : 0.07250851548262202\n",
      "Epoch 19: Loss : 0.06732233792592379\n",
      "Epoch 20: Loss : 0.0627435456335062\n",
      "Epoch 21: Loss : 0.05867651334795007\n",
      "Epoch 22: Loss : 0.05504418516716161\n",
      "Epoch 23: Loss : 0.05178383815796554\n",
      "Epoch 24: Loss : 0.04884394683988703\n",
      "Epoch 25: Loss : 0.04618182944902414\n",
      "Epoch 26: Loss : 0.04376185854202538\n",
      "Epoch 27: Loss : 0.04155408493952031\n",
      "Epoch 28: Loss : 0.03953316839466315\n",
      "Epoch 29: Loss : 0.03767753859110742\n",
      "Epoch 30: Loss : 0.03596873099319936\n",
      "Epoch 31: Loss : 0.03439085676910589\n",
      "Epoch 32: Loss : 0.03293017647512872\n",
      "Epoch 33: Loss : 0.03157475473674207\n",
      "Epoch 34: Loss : 0.030314178665771574\n",
      "Epoch 35: Loss : 0.029139326809271913\n",
      "Epoch 36: Loss : 0.028042178444223995\n",
      "Epoch 37: Loss : 0.027015655299177065\n",
      "Epoch 38: Loss : 0.02605348950112863\n",
      "Epoch 39: Loss : 0.02515011285711016\n",
      "Epoch 40: Loss : 0.024300563588705602\n",
      "Epoch 41: Loss : 0.02350040741937999\n",
      "Epoch 42: Loss : 0.02274567052426039\n",
      "Epoch 43: Loss : 0.02203278233073988\n",
      "Epoch 44: Loss : 0.021358526536404927\n",
      "Epoch 45: Loss : 0.020719999011186804\n",
      "Epoch 46: Loss : 0.020114571490593425\n",
      "Epoch 47: Loss : 0.019539860159557297\n",
      "Epoch 48: Loss : 0.018993698381923732\n",
      "Epoch 49: Loss : 0.018474112956679094\n",
      "Epoch 50: Loss : 0.01797930338471035\n",
      "Epoch 51: Loss : 0.017507623713898513\n",
      "Epoch 52: Loss : 0.01705756659936232\n",
      "Epoch 53: Loss : 0.01662774927259448\n",
      "Epoch 54: Loss : 0.016216901160361556\n",
      "Epoch 55: Loss : 0.015823852933409002\n",
      "Epoch 56: Loss : 0.015447526797676582\n",
      "Epoch 57: Loss : 0.01508692786806703\n",
      "Epoch 58: Loss : 0.014741136487761033\n",
      "Epoch 59: Loss : 0.014409301375405002\n",
      "Epoch 60: Loss : 0.014090633498827424\n",
      "Epoch 61: Loss : 0.013784400587781093\n",
      "Epoch 62: Loss : 0.013489922209966868\n",
      "Epoch 63: Loss : 0.013206565344615048\n",
      "Epoch 64: Loss : 0.012933740396462473\n",
      "Epoch 65: Loss : 0.0126708976002967\n",
      "Epoch 66: Loss : 0.0124175237725353\n",
      "Epoch 67: Loss : 0.012173139371729143\n",
      "Epoch 68: Loss : 0.01193729583455681\n",
      "Epoch 69: Loss : 0.01170957315791881\n",
      "Epoch 70: Loss : 0.011489577701248602\n",
      "Epoch 71: Loss : 0.01127694018619944\n",
      "Epoch 72: Loss : 0.011071313873516543\n",
      "Epoch 73: Loss : 0.01087237289921353\n",
      "Epoch 74: Loss : 0.010679810754193152\n",
      "Epoch 75: Loss : 0.010493338893216263\n",
      "Epoch 76: Loss : 0.010312685460676011\n",
      "Epoch 77: Loss : 0.010137594121994536\n",
      "Epoch 78: Loss : 0.00996782299065838\n",
      "Epoch 79: Loss : 0.009803143641965762\n",
      "Epoch 80: Loss : 0.00964334020549179\n",
      "Epoch 81: Loss : 0.009488208529104016\n",
      "Epoch 82: Loss : 0.009337555408090517\n",
      "Epoch 83: Loss : 0.009191197873613648\n",
      "Epoch 84: Loss : 0.009048962535276928\n",
      "Epoch 85: Loss : 0.00891068497310769\n",
      "Epoch 86: Loss : 0.00877620917471433\n",
      "Epoch 87: Loss : 0.008645387013784991\n",
      "Epoch 88: Loss : 0.008518077766459676\n",
      "Epoch 89: Loss : 0.008394147662434171\n",
      "Epoch 90: Loss : 0.008273469467945695\n",
      "Epoch 91: Loss : 0.008155922098053284\n",
      "Epoch 92: Loss : 0.00804139025586141\n",
      "Epoch 93: Loss : 0.007929764096546534\n",
      "Epoch 94: Loss : 0.0078209389142378\n",
      "Epoch 95: Loss : 0.00771481484997428\n",
      "Epoch 96: Loss : 0.0076112966191172\n",
      "Epoch 97: Loss : 0.007510293256734599\n",
      "Epoch 98: Loss : 0.007411717879603702\n",
      "Epoch 99: Loss : 0.007315487463590144\n"
     ]
    }
   ],
   "source": [
    "mlp.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "214445f9-c289-44bf-a835-9555c8c5bd7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=0.9613638541408344),\n",
       " Value(data=-0.9705982870646318),\n",
       " Value(data=-0.9524655217950789),\n",
       " Value(data=0.9480505211287783)]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.preds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
