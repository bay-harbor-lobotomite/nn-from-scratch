{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d8f277cb-2eb4-4068-8bec-faf51e03deb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b9f2fb6-aaf2-4311-9f73-efcdca8cd698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get name dataset\n",
    "with open('names.txt', 'r') as file:\n",
    "    lines = [line.strip() for line in file]\n",
    "# get unique chars\n",
    "chars = sorted(list(set(''.join(lines))))\n",
    "stoi = {s:i+1 for i, s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s, i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6204f6b-b31b-4900-98b9-4a0020c0c72d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([27, 27])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.int32"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sz = len(stoi.items())\n",
    "N = torch.zeros([sz, sz], dtype=torch.int32)\n",
    "print(N.shape)\n",
    "N.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6869bdf-9b92-4041-95f5-5d28a3b5142a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0, 4410, 1306, 1542, 1690, 1531,  417,  669,  874,  591, 2422, 2963,\n",
       "         1572, 2538, 1146,  394,  515,   92, 1639, 2055, 1308,   78,  376,  307,\n",
       "          134,  535,  929],\n",
       "        [6640,  556,  541,  470, 1042,  692,  134,  168, 2332, 1650,  175,  568,\n",
       "         2528, 1634, 5438,   63,   82,   60, 3264, 1118,  687,  381,  834,  161,\n",
       "          182, 2050,  435],\n",
       "        [ 114,  321,   38,    1,   65,  655,    0,    0,   41,  217,    1,    0,\n",
       "          103,    0,    4,  105,    0,    0,  842,    8,    2,   45,    0,    0,\n",
       "            0,   83,    0],\n",
       "        [  97,  815,    0,   42,    1,  551,    0,    2,  664,  271,    3,  316,\n",
       "          116,    0,    0,  380,    1,   11,   76,    5,   35,   35,    0,    0,\n",
       "            3,  104,    4],\n",
       "        [ 516, 1303,    1,    3,  149, 1283,    5,   25,  118,  674,    9,    3,\n",
       "           60,   30,   31,  378,    0,    1,  424,   29,    4,   92,   17,   23,\n",
       "            0,  317,    1],\n",
       "        [3983,  679,  121,  153,  384, 1271,   82,  125,  152,  818,   55,  178,\n",
       "         3248,  769, 2675,  269,   83,   14, 1958,  861,  580,   69,  463,   50,\n",
       "          132, 1070,  181],\n",
       "        [  80,  242,    0,    0,    0,  123,   44,    1,    1,  160,    0,    2,\n",
       "           20,    0,    4,   60,    0,    0,  114,    6,   18,   10,    0,    4,\n",
       "            0,   14,    2],\n",
       "        [ 108,  330,    3,    0,   19,  334,    1,   25,  360,  190,    3,    0,\n",
       "           32,    6,   27,   83,    0,    0,  201,   30,   31,   85,    1,   26,\n",
       "            0,   31,    1],\n",
       "        [2409, 2244,    8,    2,   24,  674,    2,    2,    1,  729,    9,   29,\n",
       "          185,  117,  138,  287,    1,    1,  204,   31,   71,  166,   39,   10,\n",
       "            0,  213,   20],\n",
       "        [2489, 2445,  110,  509,  440, 1653,  101,  428,   95,   82,   76,  445,\n",
       "         1345,  427, 2126,  588,   53,   52,  849, 1316,  541,  109,  269,    8,\n",
       "           89,  779,  277],\n",
       "        [  71, 1473,    1,    4,    4,  440,    0,    0,   45,  119,    2,    2,\n",
       "            9,    5,    2,  479,    1,    0,   11,    7,    2,  202,    5,    6,\n",
       "            0,   10,    0],\n",
       "        [ 363, 1731,    2,    2,    2,  895,    1,    0,  307,  509,    2,   20,\n",
       "          139,    9,   26,  344,    0,    0,  109,   95,   17,   50,    2,   34,\n",
       "            0,  379,    2],\n",
       "        [1314, 2623,   52,   25,  138, 2921,   22,    6,   19, 2480,    6,   24,\n",
       "         1345,   60,   14,  692,   15,    3,   18,   94,   77,  324,   72,   16,\n",
       "            0, 1588,   10],\n",
       "        [ 516, 2590,  112,   51,   24,  818,    1,    0,    5, 1256,    7,    1,\n",
       "            5,  168,   20,  452,   38,    0,   97,   35,    4,  139,    3,    2,\n",
       "            0,  287,   11],\n",
       "        [6763, 2977,    8,  213,  704, 1359,   11,  273,   26, 1725,   44,   58,\n",
       "          195,   19, 1906,  496,    5,    2,   44,  278,  443,   96,   55,   11,\n",
       "            6,  465,  145],\n",
       "        [ 855,  149,  140,  114,  190,  132,   34,   44,  171,   69,   16,   68,\n",
       "          619,  261, 2411,  115,   95,    3, 1059,  504,  118,  275,  176,  114,\n",
       "           45,  103,   54],\n",
       "        [  33,  209,    2,    1,    0,  197,    1,    0,  204,   61,    1,    1,\n",
       "           16,    1,    1,   59,   39,    0,  151,   16,   17,    4,    0,    0,\n",
       "            0,   12,    0],\n",
       "        [  28,   13,    0,    0,    0,    1,    0,    0,    0,   13,    0,    0,\n",
       "            1,    2,    0,    2,    0,    0,    1,    2,    0,  206,    0,    3,\n",
       "            0,    0,    0],\n",
       "        [1377, 2356,   41,   99,  187, 1697,    9,   76,  121, 3033,   25,   90,\n",
       "          413,  162,  140,  869,   14,   16,  425,  190,  208,  252,   80,   21,\n",
       "            3,  773,   23],\n",
       "        [1169, 1201,   21,   60,    9,  884,    2,    2, 1285,  684,    2,   82,\n",
       "          279,   90,   24,  531,   51,    1,   55,  461,  765,  185,   14,   24,\n",
       "            0,  215,   10],\n",
       "        [ 483, 1027,    1,   17,    0,  716,    2,    2,  647,  532,    3,    0,\n",
       "          134,    4,   22,  667,    0,    0,  352,   35,  374,   78,   15,   11,\n",
       "            2,  341,  105],\n",
       "        [ 155,  163,  103,  103,  136,  169,   19,   47,   58,  121,   14,   93,\n",
       "          301,  154,  275,   10,   16,   10,  414,  474,   82,    3,   37,   86,\n",
       "           34,   13,   45],\n",
       "        [  88,  642,    1,    0,    1,  568,    0,    0,    1,  911,    0,    3,\n",
       "           14,    0,    8,  153,    0,    0,   48,    0,    0,    7,    7,    0,\n",
       "            0,  121,    0],\n",
       "        [  51,  280,    1,    0,    8,  149,    2,    1,   23,  148,    0,    6,\n",
       "           13,    2,   58,   36,    0,    0,   22,   20,    8,   25,    0,    2,\n",
       "            0,   73,    1],\n",
       "        [ 164,  103,    1,    4,    5,   36,    3,    0,    1,  102,    0,    0,\n",
       "           39,    1,    1,   41,    0,    0,    0,   31,   70,    5,    0,    3,\n",
       "           38,   30,   19],\n",
       "        [2007, 2143,   27,  115,  272,  301,   12,   30,   22,  192,   23,   86,\n",
       "         1104,  148, 1826,  271,   15,    6,  291,  401,  104,  141,  106,    4,\n",
       "           28,   23,   78],\n",
       "        [ 160,  860,    4,    2,    2,  373,    0,    1,   43,  364,    2,    2,\n",
       "          123,   35,    4,  110,    2,    0,   32,    4,    4,   73,    2,    3,\n",
       "            1,  147,   45]], dtype=torch.int32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create bigram matrix\n",
    "for line in lines:\n",
    "    chars = ['.'] + list(line) + ['.']\n",
    "    for pr, sc in zip(chars, chars[1:]):\n",
    "        idx1 = stoi[pr]\n",
    "        idx2 = stoi[sc]\n",
    "        N[idx1][idx2] += 1\n",
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92c0ae56-f930-471a-a1c9-ee2675b88924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# demonstration of broadcasting semantics and associated nuances\n",
    "# we need the whole row to have the sum of the row for the row to be converted to a distribution\n",
    "# so its important how we broadcast the tensor\n",
    "dist = N.float()\n",
    "sumarr = dist.sum(1, keepdim=True)\n",
    "bad_sumarr = dist.sum(1)\n",
    "# print(sumarr.shape)\n",
    "# print(bad_sumarr.shape)\n",
    "dummy_arr = torch.zeros([sz, sz])\n",
    "correct_arr = dummy_arr + sumarr\n",
    "# print(correct_arr)\n",
    "bad_dist = dummy_arr + bad_sumarr\n",
    "# print(bad_dist)\n",
    "\n",
    "#actual useful code\n",
    "dist = dist / sumarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1efb6c4c-f7bc-46d0-9881-27e0543f1a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shasofz.\n",
      "ki.\n",
      "khanan.\n",
      "nyci.\n",
      "adenee.\n",
      "zilelyllyze.\n",
      "aron.\n",
      "cyri.\n",
      "dazyia.\n",
      "le.\n",
      "hirixtilamizaretaynnay.\n",
      "re.\n",
      "jaxaynn.\n",
      "jaynsolosailoeyliomana.\n",
      "lydivime.\n",
      "bera.\n",
      "h.\n",
      "jageni.\n",
      "ko.\n",
      "be.\n"
     ]
    }
   ],
   "source": [
    "# sample using multinomial\n",
    "\n",
    "for i in range(20):\n",
    "    outvec= []\n",
    "    idx = 0\n",
    "    while True:\n",
    "        d = dist[idx]\n",
    "        idx = torch.multinomial(d, num_samples=1, replacement=True).item()\n",
    "        outvec.append(itos[idx])\n",
    "        if idx == 0:\n",
    "            break\n",
    "    print(''.join(outvec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a385b412-377c-45ed-86a1-784e33acf67f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "228146"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now, we move from the hand computed probability approach to using a single layer of neurons\n",
    "# loss - average negative loss likelihood of next character, we train weights to push these predictions closer to correct\n",
    "# probabilities\n",
    "# create the training dataset - feature - prev character, label - next character\n",
    "xs, ys = [], []\n",
    "for line in lines:\n",
    "    chars = ['.'] + list(line) + ['.']\n",
    "    for ch1, ch2 in zip(chars, chars[1:]):\n",
    "        idx1 = stoi[ch1]\n",
    "        idx2 = stoi[ch2]\n",
    "        xs.append(idx1)\n",
    "        ys.append(idx2)\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "xnum = xs.nelement()\n",
    "xnum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b1e420eb-f0fb-4153-84de-a15edeff532b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8103139400482178\n",
      "3.176689386367798\n",
      "2.9380834102630615\n",
      "2.8131165504455566\n",
      "2.7476646900177\n",
      "2.7000184059143066\n",
      "2.679823875427246\n",
      "2.63059663772583\n",
      "2.6152689456939697\n",
      "2.601663827896118\n",
      "2.6081130504608154\n",
      "2.575822353363037\n",
      "2.5718016624450684\n",
      "2.565399408340454\n",
      "2.57899808883667\n",
      "2.550689935684204\n",
      "2.5504701137542725\n",
      "2.5458931922912598\n",
      "2.562288522720337\n",
      "2.5354316234588623\n"
     ]
    }
   ],
   "source": [
    "# initialize the layer and train\n",
    "W = torch.randn((sz, sz), requires_grad=True)\n",
    "num_epochs = 20\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    xenc = F.one_hot(xs, num_classes=sz).float()\n",
    "    logits = xenc @ W\n",
    "    # softmax essentially\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(1, keepdims=True)\n",
    "    # clever stuff, this will mean fuck all on a revisit unless you know the semantics we're using here\n",
    "    loss = -probs[torch.arange(xnum), ys].log().mean()\n",
    "    print(loss.item())\n",
    "\n",
    "    #backprop\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    W.data += -100*W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "803e500e-fdf0-4bb3-86dc-2e1a682223bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mdianaranajpvocpfa.\n",
      "bwe.\n",
      "pshakiiogel.\n",
      "avivarieonariaynnay.\n",
      "viartronanlenixxtrxfikkamari.\n",
      "chariariayninnan.\n",
      "sjass.\n",
      "vianademan.\n",
      "gkaikenah.\n",
      "mi.\n"
     ]
    }
   ],
   "source": [
    "# sample and generate\n",
    "\n",
    "for i in range(10):\n",
    "\n",
    "    out = []\n",
    "    ix = 0\n",
    "    while True:\n",
    "        # you only need it to have the index\n",
    "        xenc = F.one_hot(torch.tensor([ix]), num_classes=sz).float()\n",
    "        logits = xenc @ W\n",
    "        counts = logits.exp()\n",
    "        p = counts / counts.sum(1, keepdims = True)\n",
    "\n",
    "        #sample\n",
    "        ix = torch.multinomial(p, num_samples=1, replacement=True).item()\n",
    "        out.append(itos[ix])\n",
    "        if(ix == 0):\n",
    "            break\n",
    "    print(''.join(out))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
